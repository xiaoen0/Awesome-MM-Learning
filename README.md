# Awesome-MM-Learning
## Image-Language
- **4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities** <br>
[[paper](https://arxiv.org/abs/2406.09406)] [[code](https://github.com/apple/ml-4m)]
- **Minigpt-4: Enhancing vision-language understanding with advanced large language models** <br>
[ICLR 2023] [[paper](https://arxiv.org/abs/2304.10592)] [[project page](https://minigpt-4.github.io/)]
- **GILL: Generating Images with Multimodal Language Models** <br>
[NeurIPS 2023] [[paper](https://arxiv.org/abs/2305.17216)] [[code](https://github.com/kohjingyu/gill/?tab=readme-ov-file)]
- **BLIP-2** <br>
[[paper](https://arxiv.org/abs/2301.12597)]
- **Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge** <br>
[[paper](https://arxiv.org/abs/2407.04681)]
- **Robust Multimodal Learning via Representation Decoupling** <br>
[[paper](https://arxiv.org/abs/2407.04458)] <br>

## Image/Video-Text-Audio-Depth-Thermal-IMU
- **PandaGPT: One Model To Instruction-Follow Them All** <br>
[TLLM 2023] [[paper](https://arxiv.org/abs/2305.16355)] [[project page](https://panda-gpt.github.io/)] [[code](https://github.com/yxuansu/PandaGPT)] <br>
**Task**: <br>
(1) `Image-Text Tasks`: image description generation <br> 
(2) `Video-Text Tasks`: writing stories inspired by videos <br>
(3) `Audio-Text Tasks`: answering questions about audios <br>
**Data**: 160k image-text instruction-following data released by LlaVa and MiniGPT-4 <br>
**Model**: ImageBind + Vicuna
- **Enhance the Robustness in Text-Centric Multimodal Alignments** <br>
[[paper](https://arxiv.org/abs/2407.05036)] <br>







